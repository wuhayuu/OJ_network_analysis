{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus_G = nx.read_edgelist('./nx_create/nx_graph_df_timus.csv'\n",
    "                     , create_using=nx.DiGraph()\n",
    "                     , nodetype=int\n",
    "                     , data=[('weight', int)]\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(timus_G.edges(data=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(timus_G.edges(data=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(timus_G) \n",
    "plt.figure(figsize = (15, 15)) \n",
    "node_item_id = 1001\n",
    "#networkx.draw_networkx_labels(purchasedAsinEgoGraph, pos, font_size = 18) \n",
    "edgewidth = [d['weight'] for (u, v, d) in timus_G.edges(data = True)]\n",
    "nx.draw(timus_G, pos = pos, node_size = 50, node_color='g', edge_color= edgewidth, style = 'solid')\n",
    "nx.draw_networkx_nodes(timus_G, pos = pos, nodelist= [node_item_id], node_color= 'r', node_size = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = 14\n",
    "threshold = 3\n",
    "GivenNodeIslands = nx.Graph()\n",
    "for f, t, e in timus_G.edges(data = True):\n",
    "    if e['weight'] >= threshold:\n",
    "        GivenNodeIslands.add_edge(f, t, weight = e['weight'])\n",
    "node_TrimGraph = nx.Graph(GivenNodeIslands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10)) \n",
    "neg1 = 1000\n",
    "# (1000, 1001, {'weight': 453})\n",
    "neg2 = 2056\n",
    "#(1000, 2056, {'weight': 14})\n",
    "#networkx.draw_networkx_labels(purchasedAsinEgoTrimGraph, pos, font_size = 18) \n",
    "edgewidth = [d['weight'] for (u, v, d) in node_TrimGraph.edges(data = True)]\n",
    "nx.draw_networkx_nodes(node_TrimGraph, pos = pos, nodelist= nx.nodes(node_TrimGraph), node_color='g') \n",
    "nx.draw_networkx_edges(node_TrimGraph, pos = pos, edgelist= node_TrimGraph.edges)\n",
    "nx.draw_networkx_nodes(node_TrimGraph, pos = pos, nodelist= [node_item_id], node_color= 'r') #ego node\n",
    "nx.draw_networkx_nodes(node_TrimGraph, pos = pos, nodelist= [neg1], node_color= 'b') #ego node\n",
    "nx.draw_networkx_nodes(node_TrimGraph, pos = pos, nodelist= [neg2], node_color= 'r') #ego node\n",
    "plt.show()\n",
    "#共现次数越大越近\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo[1001],demo[1000],demo[2056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_TrimGraph.edges(data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_of_nodes(node_TrimGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# pr = nx.pagerank(timus_G)\n",
    "# pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus = pd.read_csv('./nx_create/nx_graph_df_timus.csv',header=None,sep=' ',names=['source','target','weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "node_dict_degree_clus = {}\n",
    "pr = nx.pagerank(timus_G)\n",
    "\n",
    "degree_centralityG = nx.degree_centrality(timus_G)\n",
    "\n",
    "eigenvector_centralityG = nx.eigenvector_centrality(timus_G,weight='weight')\n",
    "\n",
    "average_degree_connectivityG = nx.average_degree_connectivity(timus_G, weight=\"weight\")\n",
    "\n",
    "harmonic_centralityG = nx.harmonic_centrality(timus_G,distance='weight')\n",
    "\n",
    "betweenness_centralityG = nx.betweenness_centrality(timus_G,weight='weight')\n",
    "\n",
    "for node_id in tqdm(nx.nodes(timus_G)):\n",
    "    metadata = {}\n",
    "    metadata['PageRank'] = round(pr[node_id],5)\n",
    "    metadata['DegreeCentrality'] = round(degree_centralityG[node_id],5)\n",
    "    metadata['EigenvectorCentrality'] = round(eigenvector_centralityG[node_id],5)\n",
    "    metadata['HarmonicCentrality'] = round(harmonic_centralityG[node_id],5)\n",
    "    metadata['BetweennessCentrality'] = round(betweenness_centralityG[node_id],5)\n",
    "    ego = nx.ego_graph(timus_G, node_id, distance='weight')\n",
    "    metadata['ClusteringCoeff'] = round(nx.average_clustering(ego,weight='weight'),5)\n",
    "    node_dict_degree_clus[node_id] = metadata\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_dict_degree_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(node_dict_degree_clus, open( 'timus_node_dict_Centrality_clus.pkl', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = pd.DataFrame(node_dict_degree_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_dict_degree_clus.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timus_sna = []\n",
    "for item_id,metadata in tqdm(node_dict_degree_clus.items()):\n",
    "    singledata = {}\n",
    "    singledata['item_id'] = item_id\n",
    "    singledata['PageRank'] = metadata['PageRank']\n",
    "    singledata['DegreeCentrality'] = metadata['DegreeCentrality']\n",
    "    singledata['EigenvectorCentrality'] = metadata['EigenvectorCentrality']\n",
    "    singledata['HarmonicCentrality'] = metadata['HarmonicCentrality']\n",
    "    singledata['BetweennessCentrality'] = metadata['BetweennessCentrality']\n",
    "    singledata['ClusteringCoeff'] = metadata['ClusteringCoeff']\n",
    "    timus_sna.append(singledata)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus_sna_analysis = pd.DataFrame(timus_sna)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus_sna_analysis_item_id = timus_sna_analysis.set_index('item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus_sna_analysis_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_item_id_Neighbors = [n for n in node_TrimGraph.neighbors(node_item_id)]\n",
    "node_item_id_Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sanple_item_id_Neighbors = [n for n in node_TrimGraph.neighbors(node_item_id)]\n",
    "sanple_item_id_Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timus_sna_analysis_item_id.loc[sanple_item_id_Neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "timus_sna_analysis_item_id.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "timus_sna_analysis['HarmonicCentrality'] = timus_sna_analysis[['HarmonicCentrality']].apply(max_min_scaler)\n",
    "timus_sna_analysis_item_id['HarmonicCentrality'] = timus_sna_analysis_item_id[['HarmonicCentrality']].apply(max_min_scaler)\n",
    "\n",
    "\n",
    "##\n",
    "timus_sna_analysis['PageRank'] = timus_sna_analysis[['PageRank']].apply(max_min_scaler)\n",
    "timus_sna_analysis_item_id['PageRank'] = timus_sna_analysis_item_id[['PageRank']].apply(max_min_scaler)\n",
    "\n",
    "##\n",
    "timus_sna_analysis['DegreeCentrality'] = timus_sna_analysis[['DegreeCentrality']].apply(max_min_scaler)\n",
    "timus_sna_analysis_item_id['DegreeCentrality'] = timus_sna_analysis_item_id[['DegreeCentrality']].apply(max_min_scaler)\n",
    "##\n",
    "\n",
    "timus_sna_analysis['EigenvectorCentrality'] = timus_sna_analysis[['EigenvectorCentrality']].apply(max_min_scaler)\n",
    "timus_sna_analysis_item_id['EigenvectorCentrality'] = timus_sna_analysis_item_id[['EigenvectorCentrality']].apply(max_min_scaler)\n",
    "#\n",
    "timus_sna_analysis['ClusteringCoeff'] = timus_sna_analysis[['ClusteringCoeff']].apply(max_min_scaler)\n",
    "timus_sna_analysis_item_id['ClusteringCoeff'] = timus_sna_analysis_item_id[['ClusteringCoeff']].apply(max_min_scaler)\n",
    "#\n",
    "# BetweennessCentrality\n",
    "timus_sna_analysis['BetweennessCentrality'] = timus_sna_analysis[['BetweennessCentrality']].apply(max_min_scaler)\n",
    "timus_sna_analysis_item_id['BetweennessCentrality'] = timus_sna_analysis_item_id[['BetweennessCentrality']].apply(max_min_scaler)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.fit(timus_sna_analysis[['PageRank','DegreeCentrality','EigenvectorCentrality','HarmonicCentrality','BetweennessCentrality','ClusteringCoeff']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo1.loc[1001,:].values.reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = neigh.kneighbors(timus_sna_analysis_item_id.loc[1409,:].values.reshape((1,-1)),10, return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timus_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall_item_num = 11\n",
    "timus = pd.read_csv('./nx_create/nx_graph_df_timus.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# timus = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "val_source_target = []\n",
    "for _ ,group in timus.groupby('source'):\n",
    "#     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "    tmp = group.iloc[0,:]\n",
    "    val_source_target.append((tmp.source,tmp.target))\n",
    "val_source_target\n",
    "source_target = list(zip(timus.source.tolist(),timus.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(timus_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh.kneighbors(timus_sna_analysis_item_id.loc[iid,:].values.reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = timus_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[1:]\n",
    "#     print(item_recall_list)\n",
    "\n",
    "# eva_data = []\n",
    "# for iid,group in timus.groupby('source'):\n",
    "#     if len(group)>10:\n",
    "#         tmp = list(zip(group.source,group.target))[:10]\n",
    "#     else: tmp = list(zip(group.source,group.target))\n",
    "#     eva_data.extend(tmp[:])   \n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "#         for rank,item in enumerate(tmp_recall_items):\n",
    "#             for s,t in eva_data:\n",
    "#                 if s==iid and item==t:\n",
    "# #                     print(\"标注：\",s,t)\n",
    "#                     score = 1.0/(rank+1.0)\n",
    "\n",
    "#                     print(\"标注\",s,t)\n",
    "#                     break\n",
    "            \n",
    "#     for iid,recall_list in item_recall_list:\n",
    "#         tmp_recall_items = item_recall_list[iid][:k]\n",
    "#         for rank,item in enumerate(tmp_recall_items):\n",
    "#             for s,t in source_target:\n",
    "#                 if s==iid and item==t:\n",
    "#                     score += 1.0/(rank+1.0)\n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###scaler\n",
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,1,figsize=(36,36))\n",
    "nx.draw(timus_G,with_labels=True, font_weight='bold',width = 0.5)\n",
    "# nx.draw(timus_G, font_weight='bold',width = 0.5)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh1 = NearestNeighbors(n_neighbors=1)\n",
    "neigh1.fit(timus_sna_analysis[['DegreeCentrality','EigenvectorCentrality','HarmonicCentrality','BetweennessCentrality']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_item_num = 11\n",
    "timus = pd.read_csv('./nx_create/nx_graph_df_timus.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# timus = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "val_source_target = []\n",
    "for _ ,group in timus.groupby('source'):\n",
    "#     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "    tmp = group.iloc[0,:]\n",
    "    val_source_target.append((tmp.source,tmp.target))\n",
    "val_source_target\n",
    "source_target = list(zip(timus.source.tolist(),timus.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(timus_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh1.kneighbors(timus_sna_analysis_item_id.loc[iid,['DegreeCentrality','EigenvectorCentrality','HarmonicCentrality','BetweennessCentrality']].values.reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = timus_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[1:]\n",
    "#     print(item_recall_list)\n",
    "\n",
    "# eva_data = []\n",
    "# for iid,group in timus.groupby('source'):\n",
    "#     if len(group)>10:\n",
    "#         tmp = list(zip(group.source,group.target))[:10]\n",
    "#     else: tmp = list(zip(group.source,group.target))\n",
    "#     eva_data.extend(tmp[:])   \n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "#         for rank,item in enumerate(tmp_recall_items):\n",
    "#             for s,t in eva_data:\n",
    "#                 if s==iid and item==t:\n",
    "# #                     print(\"标注：\",s,t)\n",
    "#                     score = 1.0/(rank+1.0)\n",
    "\n",
    "#                     print(\"标注\",s,t)\n",
    "#                     break\n",
    "            \n",
    "#     for iid,recall_list in item_recall_list:\n",
    "#         tmp_recall_items = item_recall_list[iid][:k]\n",
    "#         for rank,item in enumerate(tmp_recall_items):\n",
    "#             for s,t in source_target:\n",
    "#                 if s==iid and item==t:\n",
    "#                     score += 1.0/(rank+1.0)\n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
