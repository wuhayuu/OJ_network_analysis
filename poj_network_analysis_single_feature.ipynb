{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poj_G = nx.read_edgelist('./nx_create/nx_graph_df_poj.csv'\n",
    "                     , create_using=nx.DiGraph()\n",
    "                     , nodetype=int\n",
    "                     , data=[('weight', int)]\n",
    "                     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(poj_G.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(poj_G.edges(data=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "node_dict_degree_clus = {}\n",
    "pr = nx.pagerank(poj_G)\n",
    "\n",
    "degree_centralityG = nx.degree_centrality(poj_G)\n",
    "\n",
    "eigenvector_centralityG = nx.eigenvector_centrality(poj_G,weight='weight')\n",
    "\n",
    "average_degree_connectivityG = nx.average_degree_connectivity(poj_G, weight=\"weight\")\n",
    "\n",
    "harmonic_centralityG = nx.harmonic_centrality(poj_G,distance='weight')\n",
    "\n",
    "betweenness_centralityG = nx.betweenness_centrality(poj_G,weight='weight')\n",
    "\n",
    "for node_id in tqdm(nx.nodes(poj_G)):\n",
    "    metadata = {}\n",
    "    metadata['PageRank'] = round(pr[node_id],5)\n",
    "    metadata['DegreeCentrality'] = round(degree_centralityG[node_id],5)#度中心性\n",
    "    metadata['EigenvectorCentrality'] = round(eigenvector_centralityG[node_id],5)#特征向量中心度\n",
    "    metadata['HarmonicCentrality'] = round(harmonic_centralityG[node_id],5)#紧密中心性 closses\n",
    "    metadata['BetweennessCentrality'] = round(betweenness_centralityG[node_id],5)#介数中心性 betweenness\n",
    "    ego = nx.ego_graph(poj_G, node_id, distance='weight')\n",
    "    metadata['ClusteringCoeff'] = round(nx.average_clustering(ego,weight='weight'),5)#聚类系数\n",
    "    node_dict_degree_clus[node_id] = metadata\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(node_dict_degree_clus, open( 'poj_node_dict_Centrality_clus_single_features.pkl', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_dict_degree_clus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poj_sna = []\n",
    "for item_id,metadata in tqdm(node_dict_degree_clus.items()):\n",
    "    singledata = {}\n",
    "    singledata['item_id'] = item_id\n",
    "    singledata['PageRank'] = metadata['PageRank']\n",
    "    singledata['DegreeCentrality'] = metadata['DegreeCentrality']\n",
    "    singledata['EigenvectorCentrality'] = metadata['EigenvectorCentrality']\n",
    "    singledata['HarmonicCentrality'] = metadata['HarmonicCentrality']\n",
    "    singledata['BetweennessCentrality'] = metadata['BetweennessCentrality']\n",
    "    singledata['ClusteringCoeff'] = metadata['ClusteringCoeff']\n",
    "    poj_sna.append(singledata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poj_sna_analysis = pd.DataFrame(poj_sna)\n",
    "poj_sna_analysis_item_id = poj_sna_analysis.set_index('item_id')\n",
    "poj_sna_analysis_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "poj_sna_analysis['HarmonicCentrality'] = poj_sna_analysis[['HarmonicCentrality']].apply(max_min_scaler)\n",
    "poj_sna_analysis_item_id['HarmonicCentrality'] = poj_sna_analysis_item_id[['HarmonicCentrality']].apply(max_min_scaler)\n",
    "\n",
    "\n",
    "##\n",
    "poj_sna_analysis['PageRank'] = poj_sna_analysis[['PageRank']].apply(max_min_scaler)\n",
    "poj_sna_analysis_item_id['PageRank'] = poj_sna_analysis_item_id[['PageRank']].apply(max_min_scaler)\n",
    "\n",
    "##\n",
    "poj_sna_analysis['DegreeCentrality'] = poj_sna_analysis[['DegreeCentrality']].apply(max_min_scaler)\n",
    "poj_sna_analysis_item_id['DegreeCentrality'] = poj_sna_analysis_item_id[['DegreeCentrality']].apply(max_min_scaler)\n",
    "##\n",
    "\n",
    "poj_sna_analysis['EigenvectorCentrality'] = poj_sna_analysis[['EigenvectorCentrality']].apply(max_min_scaler)\n",
    "poj_sna_analysis_item_id['EigenvectorCentrality'] = poj_sna_analysis_item_id[['EigenvectorCentrality']].apply(max_min_scaler)\n",
    "#\n",
    "poj_sna_analysis['ClusteringCoeff'] = poj_sna_analysis[['ClusteringCoeff']].apply(max_min_scaler)\n",
    "poj_sna_analysis_item_id['ClusteringCoeff'] = poj_sna_analysis_item_id[['ClusteringCoeff']].apply(max_min_scaler)\n",
    "#\n",
    "# BetweennessCentrality\n",
    "poj_sna_analysis['BetweennessCentrality'] = poj_sna_analysis[['BetweennessCentrality']].apply(max_min_scaler)\n",
    "poj_sna_analysis_item_id['BetweennessCentrality'] = poj_sna_analysis_item_id[['BetweennessCentrality']].apply(max_min_scaler)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigh.fit(poj_sna_analysis[['PageRank','DegreeCentrality','EigenvectorCentrality','HarmonicCentrality','BetweennessCentrality','ClusteringCoeff']].values)\n",
    "neigh.fit(poj_sna_analysis[['PageRank']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pagerank\n",
    "\n",
    "recall_item_num = 11\n",
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# poj = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "val_source_target = []\n",
    "for _ ,group in poj.groupby('source'):\n",
    "#     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "    tmp = group.iloc[0,:]\n",
    "    val_source_target.append((tmp.source,tmp.target))\n",
    "val_source_target\n",
    "source_target = list(zip(poj.source.tolist(),poj.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(poj_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh.kneighbors(poj_sna_analysis_item_id.loc[iid,'PageRank'].reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = poj_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[:]\n",
    "#     print(item_recall_list)\n",
    "\n",
    "# eva_data = []\n",
    "# for iid,group in poj.groupby('source'):\n",
    "#     if len(group)>10:\n",
    "#         tmp = list(zip(group.source,group.target))[:10]\n",
    "#     else: tmp = list(zip(group.source,group.target))\n",
    "#     eva_data.extend(tmp[:])   \n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo[1001],demo[1000],demo[2056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DegreeCentrality\n",
    "\n",
    "neigh1 = NearestNeighbors(n_neighbors=1)\n",
    "neigh1.fit(poj_sna_analysis[['DegreeCentrality']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pagerank\n",
    "\n",
    "recall_item_num = 11\n",
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# poj = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "# val_source_target = []\n",
    "# for _ ,group in poj.groupby('source'):\n",
    "# #     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "#     tmp = group.iloc[0,:]\n",
    "#     val_source_target.append((tmp.source,tmp.target))\n",
    "# val_source_target\n",
    "source_target = list(zip(poj.source.tolist(),poj.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(poj_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh1.kneighbors(poj_sna_analysis_item_id.loc[iid,'DegreeCentrality'].reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = poj_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[:]\n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EigenvectorCentrality\n",
    "neigh2 = NearestNeighbors(n_neighbors=1)\n",
    "neigh2.fit(poj_sna_analysis[['EigenvectorCentrality']].values)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pagerank\n",
    "\n",
    "recall_item_num = 11\n",
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# poj = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "# val_source_target = []\n",
    "# for _ ,group in poj.groupby('source'):\n",
    "# #     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "#     tmp = group.iloc[0,:]\n",
    "#     val_source_target.append((tmp.source,tmp.target))\n",
    "# val_source_target\n",
    "source_target = list(zip(poj.source.tolist(),poj.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(poj_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh2.kneighbors(poj_sna_analysis_item_id.loc[iid,'EigenvectorCentrality'].reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = poj_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[:]\n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HarmonicCentrality\n",
    "neigh3 = NearestNeighbors(n_neighbors=1)\n",
    "neigh3.fit(poj_sna_analysis[['HarmonicCentrality']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "recall_item_num = 11\n",
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# poj = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "# val_source_target = []\n",
    "# for _ ,group in poj.groupby('source'):\n",
    "\n",
    "# #     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "#     tmp = group.iloc[0,:]\n",
    "#     val_source_target.append((tmp.source,tmp.target))\n",
    "# val_source_target\n",
    "source_target = list(zip(poj.source.tolist(),poj.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(poj_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh3.kneighbors(poj_sna_analysis_item_id.loc[iid,'HarmonicCentrality'].reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = poj_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[:]\n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4'BetweennessCentrality','ClusteringCoeff'\n",
    "neigh4 = NearestNeighbors(n_neighbors=1)\n",
    "neigh4.fit(poj_sna_analysis[['BetweennessCentrality']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "recall_item_num = 11\n",
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# poj = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "# val_source_target = []\n",
    "# for _ ,group in poj.groupby('source'):\n",
    "# #     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "#     tmp = group.iloc[0,:]\n",
    "#     val_source_target.append((tmp.source,tmp.target))\n",
    "# val_source_target\n",
    "source_target = list(zip(poj.source.tolist(),poj.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(poj_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh4.kneighbors(poj_sna_analysis_item_id.loc[iid,'BetweennessCentrality'].reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = poj_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[:]\n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4'BetweennessCentrality','ClusteringCoeff'\n",
    "neigh5 = NearestNeighbors(n_neighbors=1)\n",
    "neigh5.fit(poj_sna_analysis[['ClusteringCoeff']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "recall_item_num = 11\n",
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "# poj = pd.read_csv('./nx_create/clean__data_full_.txt',sep='\\t')\n",
    "# val_source_target = []\n",
    "# for _ ,group in poj.groupby('source'):\n",
    "# #     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "#     tmp = group.iloc[0,:]\n",
    "#     val_source_target.append((tmp.source,tmp.target))\n",
    "# val_source_target\n",
    "source_target = list(zip(poj.source.tolist(),poj.target.tolist()))\n",
    "# source_target_dict = {}\n",
    "\n",
    "item_recall_list = {}\n",
    "for iid in tqdm(poj_sna_analysis['item_id'].unique()):\n",
    "    ind = neigh5.kneighbors(poj_sna_analysis_item_id.loc[iid,'ClusteringCoeff'].reshape((1,-1)),recall_item_num, return_distance=False)\n",
    "    item_recall_list[iid] = poj_sna_analysis.loc[ind.tolist()[0]].iloc[:,0].tolist()[:]\n",
    "\n",
    "\n",
    "def metrics_recall(eva_data,item_recall_list, k=10):\n",
    "    item_num = len(item_recall_list)\n",
    "    \n",
    "    hit_num = 0\n",
    "    score=0\n",
    "    past = []\n",
    "    for iid, recall_list in tqdm(item_recall_list.items()):#从item列表中读取每一个item\n",
    "        # 获取前k个召回的结果\n",
    "        tmp_recall_items = item_recall_list[iid][:k] #返回一个item的列表\n",
    "        for s,t in eva_data:\n",
    "            if s == iid and t in list(tmp_recall_items):#在评估数据中查找是否存在推荐列表中\n",
    "#                 print(\"标注：\",s,t)\n",
    "                rank = list(tmp_recall_items).index(t)\n",
    "                score += 1.0/(rank+1.0)\n",
    "                hit_num += 1\n",
    "                break\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    ###\n",
    "            \n",
    "    mrr_score = round(score* 1.0 / item_num, 5)\n",
    "    hit_rate = round(hit_num * 1.0 / item_num, 5)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', item_num)\n",
    "    print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'mrr_score: ', mrr_score, 'user_num : ', item_num)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_recall(source_target,item_recall_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poj = pd.read_csv('./nx_create/nx_graph_df_poj.csv',header=None,sep=' ',names=['source','target','weight'])\n",
    "val_source_target = []\n",
    "for _ ,group in poj.groupby('source'):\n",
    "#     tmp = group.iloc[np.random.choice(range(len(group)),1)[0],:]\n",
    "    tmp = group.iloc[0,:]\n",
    "    val_source_target.append((tmp.source,tmp.target))\n",
    "val_source_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
